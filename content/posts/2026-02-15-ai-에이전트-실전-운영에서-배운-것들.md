---
title: "AI 에이전트 실전 운영에서 배운 것들"
date: 2026-02-15
tags: ["ai", "agent", "llm", "에이전트", "운영", "mlops"]
summary: "에이전트를 프로덕션에 올리면 진짜 문제가 시작된다. 모니터링, 비용, 환각 대응, 멀티 에이전트, 보안까지 — 실전에서 배운 교훈을 정리했다."
draft: false
---

![AI 에이전트 실전 운영](/images/ai-agent-operations.png)

## 들어가며

[1편](/posts/2026-02-15-ai-에이전트란-무엇인가/)에서 에이전트의 개념을, [2편](/posts/2026-02-15-ai-에이전트를-직접-만들어보자/)에서 직접 만드는 방법을 다뤘다. 개념을 이해하고 코드를 짜는 것까지는 생각보다 어렵지 않다.

진짜 문제는 그 다음이다. **프로덕션에 올리고 나서 시작된다.**

에이전트를 실제로 운영해보면, 개발할 때는 전혀 생각하지 못했던 문제들이 하나씩 튀어나온다. 에이전트가 같은 도구를 무한히 호출하거나, 한 달 API 비용이 예상의 5배가 나오거나, 어느 날 갑자기 없는 정보를 지어내서 외부 API에 잘못된 요청을 보내거나.

이 글은 그런 실전 경험에서 나온 이야기다. 이론적으로 이래야 한다는 얘기보다는, 실제로 운영해보니 이랬다는 얘기를 하려 한다.

## 모니터링과 로깅 — 에이전트가 뭘 하고 있는지 알아야 한다

### 에이전트는 블랙박스다

일반적인 서비스는 요청이 들어오면 정해진 로직을 따라 응답한다. 로직이 코드에 있으니 동작을 예측할 수 있다. 에이전트는 다르다. 같은 입력을 줘도 LLM의 판단에 따라 다른 도구를 호출하고, 다른 순서로 처리하고, 다른 결과를 낼 수 있다.

그래서 **모든 단계를 로깅하는 게 생존의 문제**다.

### 무엇을 기록해야 하는가

최소한 이 정도는 기록해야 한다:

```python
# 에이전트 실행 로그 구조 예시
execution_log = {
    "session_id": "abc-123",
    "timestamp": "2026-02-15T10:30:00Z",
    "user_input": "최근 일주일 매출 분석해줘",
    "steps": [
        {
            "step": 1,
            "action": "tool_call",
            "tool": "query_database",
            "input": {"query": "SELECT ..."},
            "output": {"rows": 150},
            "tokens_used": 1200,
            "latency_ms": 340
        },
        {
            "step": 2,
            "action": "tool_call",
            "tool": "run_code",
            "input": {"code": "import pandas as pd ..."},
            "output": {"result": "분석 완료"},
            "tokens_used": 800,
            "latency_ms": 520
        },
        {
            "step": 3,
            "action": "final_response",
            "content": "지난 일주일 매출은...",
            "tokens_used": 600,
            "latency_ms": 280
        }
    ],
    "total_tokens": 2600,
    "total_latency_ms": 1140,
    "status": "success"
}
```

핵심은 **각 스텝마다 무슨 도구를 왜 호출했고, 입출력이 뭐였는지**를 남기는 것이다. 나중에 "에이전트가 왜 이런 결과를 냈지?"라고 의문이 들 때, 이 로그가 유일한 단서다.

### 실행 히스토리 저장

로그를 쌓는 것만으로는 부족하다. 시간이 지나면 패턴이 보여야 한다.

- 평균 몇 스텝만에 작업을 완료하는가?
- 어떤 도구가 가장 많이 호출되는가?
- 실패율이 높은 작업 유형은?
- 토큰 사용량의 추세는?

이런 지표를 대시보드로 만들어두면, 에이전트가 "잘" 동작하고 있는지 판단할 수 있다. 실패율이 갑자기 올라가면 프롬프트가 깨졌거나 외부 API가 바뀐 거다.

## 비용 최적화 — 생각보다 많이 나온다

### 모델 라우팅(Model Routing)

에이전트는 LLM을 여러 번 호출하기 때문에, 모든 호출에 최상위 모델을 쓰면 비용이 폭발한다. 현실적인 해결책은 **작업 복잡도에 따라 모델을 나눠 쓰는 것**이다.

{{< mermaid >}}
flowchart LR
    Input[사용자 요청] --> Router{복잡도 판단}
    Router -->|단순 질문<br/>분류/요약| Small[경량 모델<br/>GPT-4o mini 등]
    Router -->|복잡한 추론<br/>코드 생성| Large[고성능 모델<br/>GPT-4o / Claude 등]
    Router -->|멀티스텝 분석<br/>아키텍처 설계| Premium[최상위 모델<br/>o1 / Opus 등]
    Small --> Response[응답]
    Large --> Response
    Premium --> Response
{{< /mermaid >}}

복잡도 판단은 여러 방법이 있다:

```python
def route_model(task: str, context: dict) -> str:
    """작업 복잡도에 따라 모델을 선택한다"""

    # 규칙 기반 라우팅
    simple_patterns = ["날씨", "시간", "단위 변환", "간단한 질문"]
    complex_patterns = ["분석", "코드 작성", "비교", "설계"]

    if any(p in task for p in simple_patterns):
        return "gpt-4o-mini"
    elif any(p in task for p in complex_patterns):
        return "gpt-4o"
    else:
        return "gpt-4o"  # 기본값
```

규칙 기반으로 시작하고, 로그 데이터가 쌓이면 분류기로 발전시킬 수 있다. 실제로 모델 라우팅만 잘 해도 **비용을 50~70% 절감**할 수 있다.

### 캐싱

같은 질문이 반복되면 LLM을 다시 호출할 필요가 없다. 정확히 같은 질문이 아니더라도, 의미적으로 유사한 질문은 캐싱할 수 있다.

```python
import hashlib

def get_cache_key(messages: list) -> str:
    """메시지 리스트에서 캐시 키를 생성한다"""
    content = json.dumps(messages, sort_keys=True)
    return hashlib.sha256(content.encode()).hexdigest()

cache = {}

def call_llm_with_cache(messages: list, model: str):
    key = get_cache_key(messages)
    if key in cache:
        return cache[key]

    response = client.chat.completions.create(model=model, messages=messages)
    cache[key] = response
    return response
```

프로덕션에서는 Redis나 DB에 TTL(유효기간)을 걸어서 관리한다. 실시간 정보가 필요한 질문은 캐싱하면 안 되니 주의.

### 토큰 절약 팁

- **시스템 프롬프트를 간결하게** — 매 호출마다 포함되니 불필요한 내용 제거
- **도구 실행 결과를 요약** — DB에서 100행을 가져왔다면 전부 넘기지 말고 요약해서 전달
- **대화 히스토리 관리** — 오래된 턴은 요약해서 압축하거나, 관련 없는 턴은 제거

## 에이전트가 실수했을 때 — 그리고 반드시 실수한다

### 환각(Hallucination) 대응

LLM이 없는 정보를 만들어내는 건 챗봇에서도 문제지만, 에이전트에서는 **실제 행동으로 이어지기 때문에** 훨씬 위험하다.

예를 들어 "상품 ID 12345의 가격을 업데이트해줘"라는 요청에서 에이전트가 상품 ID를 확인하지 않고 임의의 ID로 DB를 수정하면? 데이터가 오염된다.

대응 방법:

1. **도구 실행 전 입력 검증** — 존재하는 ID인지, 범위 내 값인지 체크
2. **쓰기 작업 전 읽기 작업** — "먼저 조회하고 확인한 뒤 수정"하도록 프롬프트에 명시
3. **위험한 작업은 dry-run** — 실제 실행 전에 "이렇게 할 건데 맞나?" 단계를 추가

### 무한 루프 방지

2편에서 `MAX_ITERATIONS`를 설정하라고 했는데, 실전에서는 그것만으로 부족하다. 좀 더 정교한 방어가 필요하다:

```python
def detect_loop(steps: list, window: int = 3) -> bool:
    """최근 N개 스텝이 같은 패턴을 반복하는지 감지한다"""
    if len(steps) < window * 2:
        return False

    recent = [(s["tool"], s.get("input")) for s in steps[-window:]]
    previous = [(s["tool"], s.get("input")) for s in steps[-window*2:-window]]

    return recent == previous
```

같은 도구를 같은 파라미터로 반복 호출하고 있다면 루프에 빠진 거다. 이걸 감지해서 에이전트에게 "너 반복하고 있어, 다른 방법을 시도해"라고 알려주거나, 사람에게 에스컬레이션한다.

### Human-in-the-loop

완전 자율 에이전트는 아직 위험하다. 중요한 결정 포인트에서는 사람이 개입해야 한다.

```python
REQUIRES_APPROVAL = ["delete_file", "send_email", "modify_database", "external_api_call"]

def execute_with_approval(tool_name: str, args: dict) -> str:
    if tool_name in REQUIRES_APPROVAL:
        print(f"⚠️ 승인 필요: {tool_name}({args})")
        approval = input("실행할까요? (y/n): ")
        if approval.lower() != "y":
            return json.dumps({"status": "rejected", "reason": "사용자가 거부함"})

    return execute_tool(tool_name, args)
```

실시간으로 승인받기 어려운 환경이면, **위험 등급을 나눠서** 낮은 위험은 자동 실행, 높은 위험은 큐에 넣고 나중에 승인받는 방식도 가능하다.

### 롤백

에이전트가 뭔가를 실행한 뒤 되돌려야 할 때를 대비해야 한다. 모든 쓰기 작업에 대해 **되돌릴 수 있는 방법을 미리 정해두는 것**이 중요하다.

- DB 수정 → 변경 전 값을 로그에 저장
- 파일 생성/수정 → 버전 관리 또는 백업
- 외부 API 호출 → 취소 API가 있으면 기록

## 멀티 에이전트 패턴 — 혼자서는 한계가 있다

작업이 복잡해지면 에이전트 하나로는 감당하기 어렵다. 시스템 프롬프트가 길어지고, 도구가 많아지면 LLM의 판단 정확도가 떨어진다. 이럴 때 **멀티 에이전트 패턴**을 쓴다.

### 메인 + 서브 에이전트

가장 실용적인 패턴이다. 메인 에이전트가 전체 작업을 관장하고, 세부 작업은 서브 에이전트에게 위임한다.

{{< mermaid >}}
flowchart TB
    User[👤 사용자] --> Main[🤖 메인 에이전트<br/>작업 분해 & 위임]
    Main -->|데이터 분석 요청| Sub1[📊 분석 에이전트<br/>DB 쿼리 + 통계]
    Main -->|코드 작성 요청| Sub2[💻 코딩 에이전트<br/>코드 생성 + 테스트]
    Main -->|보고서 작성 요청| Sub3[📝 작성 에이전트<br/>문서 정리 + 포매팅]
    Sub1 -->|결과 반환| Main
    Sub2 -->|결과 반환| Main
    Sub3 -->|결과 반환| Main
    Main --> Response[최종 응답]
{{< /mermaid >}}

이 패턴의 장점:

- **각 에이전트의 프롬프트가 짧아진다** — 역할이 명확하니 성능이 올라감
- **도구를 분리할 수 있다** — 분석 에이전트에만 DB 접근 권한을 주고, 코딩 에이전트에는 파일 시스템 접근만 주는 식
- **모델도 분리할 수 있다** — 단순 정리는 경량 모델, 복잡한 추론은 고성능 모델

### 위임 시 컨텍스트 전달

서브 에이전트에게 작업을 위임할 때 가장 많이 하는 실수가 **컨텍스트 누락**이다. 메인 에이전트의 대화 히스토리를 서브 에이전트가 알 수 없으니, 필요한 정보를 명시적으로 전달해야 한다.

```python
def delegate_to_sub_agent(task: str, context: dict) -> str:
    """서브 에이전트에게 작업을 위임한다"""
    sub_prompt = f"""
    ## 작업
    {task}

    ## 배경 정보
    - 사용자가 원하는 것: {context['user_goal']}
    - 지금까지 진행된 것: {context['completed_steps']}
    - 참고 데이터: {context['relevant_data']}

    ## 제약 사항
    - 최대 {context.get('max_steps', 5)}스텝 내로 완료할 것
    - 결과는 JSON 형식으로 반환할 것
    """
    return run_agent(sub_prompt, model=context.get('model', 'gpt-4o'))
```

### 병렬 처리

독립적인 작업은 병렬로 실행하면 시간을 크게 줄일 수 있다. "3개 데이터소스에서 각각 데이터를 가져와서 합쳐줘"라면, 3개를 동시에 호출하는 게 당연히 빠르다.

```python
import asyncio

async def parallel_agents(tasks: list[dict]) -> list:
    """여러 서브 에이전트를 병렬로 실행한다"""
    async def run_one(task):
        return await async_run_agent(task["prompt"], model=task["model"])

    results = await asyncio.gather(*[run_one(t) for t in tasks])
    return results
```

다만 병렬 실행 시 **결과를 합치는 로직**이 중요하다. 각 서브 에이전트의 결과를 메인 에이전트가 종합해서 최종 응답을 만들어야 한다.

## 보안 고려사항 — 에이전트는 권한이 있다

에이전트는 도구를 통해 실제 시스템에 접근한다. 이건 일반 챗봇과 근본적으로 다른 보안 모델이 필요하다는 뜻이다.

### 최소 권한 원칙

에이전트에게 필요 이상의 권한을 주지 마라.

- DB 접근이 필요하면 **읽기 전용 계정**으로 시작
- 파일 시스템 접근은 **특정 디렉토리로 제한**
- 외부 API 호출은 **화이트리스트 방식**으로 관리

```python
# 도구 권한 정의 예시
TOOL_PERMISSIONS = {
    "query_database": {
        "allowed": True,
        "mode": "read_only",
        "allowed_tables": ["products", "categories"]
    },
    "write_file": {
        "allowed": True,
        "allowed_paths": ["/tmp/agent_output/"]
    },
    "delete_file": {
        "allowed": False  # 아예 비활성화
    }
}
```

### 민감정보 보호

에이전트에게 넘기는 데이터에 API 키, 비밀번호, 개인정보가 포함되면 안 된다. LLM 프로바이더에 데이터가 전송되기 때문이다.

- 환경 변수로 관리하고 에이전트에게는 도구를 통해서만 접근하게 한다
- 도구 실행 결과에 민감정보가 포함되면 마스킹해서 LLM에 넘긴다
- 로그에도 민감정보가 남지 않도록 필터링한다

### 외부 행동 전 승인

에이전트가 외부 시스템에 영향을 미치는 행동(이메일 전송, API 호출, 데이터 수정)을 하기 전에는 **반드시 승인 단계**를 넣어야 한다. 이건 앞에서 다룬 Human-in-the-loop의 핵심이기도 하다.

에이전트의 자율성은 **내부 판단**에서 발휘되어야 한다. 외부에 영향을 미치는 행동은 사람이 최종 확인하는 구조가 안전하다.

## 앞으로의 방향

### MCP(Model Context Protocol)

2024년 말 Anthropic이 공개한 MCP는 에이전트가 외부 도구와 소통하는 방식을 표준화하려는 시도다. 지금은 각 프레임워크마다 도구 정의 방식이 다른데, MCP는 이걸 통일하려 한다.

USB가 기기마다 다른 커넥터를 하나로 통일한 것처럼, MCP는 에이전트-도구 연결을 표준화하려는 프로토콜이다. 아직 초기 단계지만, 이런 표준화가 진행될수록 에이전트 생태계가 빠르게 성장할 것이다.

### 에이전트 생태계의 미래

{{< mermaid >}}
graph LR
    subgraph 현재["현재 (2025~2026)"]
        A1[단일 에이전트<br/>+ 수동 도구 연결]
        A2[프레임워크 난립<br/>LangChain, CrewAI, ...]
    end

    subgraph 가까운미래["가까운 미래"]
        B1[표준화된 도구 프로토콜<br/>MCP 등]
        B2[에이전트 마켓플레이스<br/>플러그인 생태계]
    end

    subgraph 먼미래["먼 미래"]
        C1[에이전트간 협업 프로토콜]
        C2[자율 에이전트 네트워크]
    end

    현재 --> 가까운미래 --> 먼미래
{{< /mermaid >}}

개인적으로 주목하는 방향들:

- **에이전트 평가(Eval) 표준화** — 에이전트가 "잘" 동작하는지 측정하는 방법이 아직 정립되지 않았다. 단순 정확도가 아니라 효율성, 안전성, 비용 효율까지 포함하는 평가 체계가 필요하다.
- **경량 에이전트** — 모든 에이전트가 GPT-4 급 모델을 쓸 필요는 없다. 작은 모델로도 충분한 에이전트 패턴이 더 많이 연구될 것이다.
- **에이전트 전용 인프라** — 에이전트의 실행 상태를 관리하고, 장기 실행 작업을 추적하고, 실패 시 복구하는 인프라가 필요하다. 기존 웹 서비스 인프라와는 요구사항이 다르다.

## 시리즈를 마무리하며

3편에 걸쳐 AI 에이전트를 다뤘다.

- **[1편 — 에이전트란 무엇인가](/posts/2026-02-15-ai-에이전트란-무엇인가/)**: LLM + 도구 + 메모리 + 계획 = 에이전트. 챗봇과의 차이, 핵심 구성요소, 실행 루프를 정리했다.
- **[2편 — 직접 만들어보자](/posts/2026-02-15-ai-에이전트를-직접-만들어보자/)**: 60줄로 에이전트의 핵심을 구현했다. 도구 설계, 프롬프트 엔지니어링, 메모리 시스템까지 실전 구현법을 다뤘다.
- **3편(이 글) — 실전 운영에서 배운 것들**: 모니터링, 비용, 환각, 멀티 에이전트, 보안. 프로덕션에서 부딪히는 현실적인 문제와 대응법을 정리했다.

에이전트는 아직 완성된 기술이 아니다. 환각도 있고, 비용도 비싸고, 예측 불가능한 행동도 한다. 하지만 1년 전과 비교하면 놀라울 만큼 발전했고, 이미 많은 영역에서 실용적으로 쓰이고 있다.

마지막으로 하고 싶은 말은 이거다: **직접 만들어봐라.** 개념 글 100편 읽는 것보다 에이전트 루프 한번 돌려보는 게 훨씬 많이 배운다. 작은 것부터 시작해서, 도구를 하나씩 붙이고, 실패를 경험하고, 그 과정에서 자기만의 노하우를 쌓아가면 된다.

에이전트 시대는 이미 시작됐다. 지켜보는 것보다 참여하는 게 재밌다.
